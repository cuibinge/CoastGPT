import os
import json
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

# ----------- 1. åˆå§‹åŒ– CLIP æ¨¡åž‹å’Œé¢„å¤„ç†å™¨ -----------
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ----------- 2. è‡ªåŠ¨åŒ¹é…å›¾åƒæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šæ‰©å±•åï¼‰ -----------
def find_image_path(image_folder, base_name):
    exts = [".jpg", ".jpeg", ".png", ".tif", ".tiff"]
    for ext in exts:
        path = os.path.join(image_folder, base_name + ext)
        if os.path.exists(path):
            return path
    return None

# ----------- 3. è®¡ç®—å›¾åƒä¸Žå¤šä¸ªæ–‡æœ¬å¥å­çš„ CLIP ç›¸ä¼¼åº¦ -----------
def clip_score(image_path, sentences):
    image = Image.open(image_path).convert("RGB")
    # å¤šæ–‡æœ¬å¥å­ + 1 å¼ å›¾åƒé€å…¥æ¨¡åž‹
    inputs = processor(images=image, text=sentences, return_tensors="pt", padding=True)

    with torch.no_grad():
        outputs = model(**inputs)
        image_embeds = outputs.image_embeds    # å›¾åƒåµŒå…¥
        text_embeds = outputs.text_embeds      # æ–‡æœ¬åµŒå…¥

    # å¯¹åµŒå…¥åšå•ä½å‘é‡å½’ä¸€åŒ–ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦æ ‡å‡†æµç¨‹ï¼‰
    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)
    text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)

    # æ‰¹é‡è®¡ç®—æ¯ä¸ªæ–‡æœ¬ä¸Žå›¾åƒä¹‹é—´çš„ç›¸ä¼¼åº¦
    similarity = torch.matmul(text_embeds, image_embeds.T).squeeze()  # shape: [num_sentences]
    return similarity.tolist()

# ----------- 4. ä¸»æµç¨‹ï¼šè¯»å– JSONï¼Œå¤„ç†å›¾åƒå’Œæ–‡æœ¬ -----------
def process_json(json_path, image_folder):
    with open(json_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)

    results = []

    for item in json_data["data"]:
        name = item["name"]
        caption = item.get("caption", "").strip()

        if not caption:
            print(f"âš ï¸ è·³è¿‡æ—  caption çš„å›¾åƒ: {name}")
            continue

        # åŒ¹é…å›¾åƒè·¯å¾„
        image_path = find_image_path(image_folder, name)
        if not image_path:
            print(f"âŒ æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶: {name}ï¼ˆå°è¯•äº†å¤šç§æ‰©å±•åï¼‰")
            continue

        # æ‹†åˆ† caption ä¸ºå¤šä¸ªå¥å­ï¼ˆå¥å·åˆ†éš”ï¼‰
        sentences = [s.strip() for s in caption.split(".") if s.strip()]
        sentences = sentences[:5]  # åªå–å‰ 5 ä¸ªå¥å­

        if not sentences:
            print(f"âš ï¸ æ— æœ‰æ•ˆå¥å­: {name}")
            continue

        # ä½¿ç”¨ CLIP æ‰“åˆ†
        scores = clip_score(image_path, sentences)

        # ä¿å­˜ç»“æžœ
        results.append({
            "name": name,
            "sentences": sentences,
            "scores": scores
        })

    return results

# ----------- 5. ç¨‹åºå…¥å£ï¼šæŒ‡å®šè·¯å¾„å¹¶è¿è¡Œä¸»é€»è¾‘ -----------
if __name__ == "__main__":
    image_folder = r"C:\Users\me\Desktop\images"     # å›¾åƒç›®å½•è·¯å¾„
    json_path = "RS_Caption_Dataset.json"            # æ³¨é‡Š JSON è·¯å¾„

    # æ‰§è¡Œå¤„ç†å‡½æ•°
    results = process_json(json_path, image_folder)

    # è¾“å‡ºæ¯å¹…å›¾åƒæ¯å¥è¯çš„ç›¸ä¼¼åº¦ç»“æžœ
    for item in results:
        print(f"\nðŸ“· {item['name']}")
        for i, (s, sc) in enumerate(zip(item["sentences"], item["scores"])):
            print(f"  [{i+1}] {s} --> ç›¸ä¼¼åº¦: {sc:.4f}")
